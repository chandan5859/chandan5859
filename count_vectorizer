from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer

# Essentially, CountVectorizer counts all the unique words in the document and creates a matrix where occurrence of each word
# is denoted by a 1, and non-occurrence of course, is denoted by a 0

# TFIDF vectorizer converts the CountVectorizer output which is a matrix of counts of each word in the document and applies
# the term-frequency/inverse document frequency formula to convert it into another matrix which will be used as input for the
# machine learning/deep learning model (refer formula)

cntizer = CountVectorizer(analyzer="word", 
                          max_features=1500, 
                          tokenizer=None,    
                          preprocessor=None, 
                          stop_words=None,  
                          max_df=0.7,
                          min_df=0.1) 
# max_features here is used to limit the occurrence of rare words that have no importance in this document (eliminating the
# possibility of sparse matrix)

X_cnt = cntizer.fit_transform(list_posts)
print("CountVectorizer output shape:")
print(X_cnt.shape)

tfizer = TfidfTransformer()
# Learn the idf vector (fit) and transform a count matrix to a tf-idf representation
X_tfidf =  tfizer.fit_transform(X_cnt).toarray()
print("Tf-idf matrix shape:")
print(X_tfidf.shape)
# Now, if we're curious we can see which were these unique words that CountVectorizer was built on

feature_names = list(enumerate(cntizer.get_feature_names()))

print(len(feature_names)) 
# observe output, there seems to be 791 unique "features" in 
# CountVectorizer matrix, so max_features value exceeded the number of unique words

feature_names
# Now, observe how the tf-idf representation of the first row looks

print("X: Posts in tf-idf representation \n* 1st row:\n%s" % X_tfidf[0])
X_tfidf[0].shape
